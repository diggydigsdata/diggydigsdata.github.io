{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Too Messy to Melt\"\n",
    "author: \"Diggy\"\n",
    "date: \"2026-01-15\"\n",
    "description: \"Downloadable attendance churn dataset and walkthrough of its creation - from messy and manual Excel to a tidy and reproducible Python pipeline.\"\n",
    "image: \"featured.png\"\n",
    "image-alt: \"A picture of Stadion Za Lužánkami, formerly the 50,000 capacity home ground of Zbrojovka Brno, and now an overgrown jungle.\"\n",
    "categories: [Football, Analytics, Data Engineering]\n",
    "license: \"CC BY\"\n",
    "jupyter: python3  \n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "\n",
    "execute:\n",
    "  eval: false\n",
    "  echo: true\n",
    "  warning: false\n",
    "  message: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In pandas (and tidy-data workflows), `melt()` is the standard tool for reshaping a **wide** table into a **long** one.  \n",
    "In this project, the original spreadsheet (see below) wasn’t just wide — it was inconsistent across season formats, team naming conventions, and placeholder values (e.g., `COVID`). A naive `pd.melt()` would have produced incorrect joins and silent errors.\n",
    "\n",
    "![Unusably messy attendance spreadsheet](../../assets/img/excel_mess.png){.lightbox}\n",
    "\n",
    "This page documents the **reproducible reshaping pipeline** used to build the final open dataset.\n",
    "\n",
    "There are two final outputs of this process, which correspond to the Silver and Gold layers of a [Medallion Architecure](https://www.databricks.com/glossary/medallion-architecture).\n",
    "\n",
    "Silver: the **perfect_tidy_data.csv** file.\n",
    "Gold: an analytics-ready parquet file.\n",
    "\n",
    "\n",
    "Read on to learn more about its structure, contents and composition or simply [Download perfect_tidy_data.csv](../../assets/files/datasets/perfect_tidy_data.csv). The CSV becomes an analysis-ready Python DataFrame after some further enrichment - see the final code block for details.\n",
    "\n",
    "## Dataset Scope\n",
    "All Premier League relegations with usable attendance data. This means Covid seasons are excluded. Each Premier League relegation event is represented across a consistent four-season window and includes league performance as well as attendance figures. Data is on a per-season basis, not per-match.\n",
    "- **Year -1**: Season before relegation  \n",
    "- **Year 0**: Relegation season (Premier League)  \n",
    "- **Year +1**: First post-relegation season  \n",
    "- **Year +2**: Second post-relegation season\n",
    "\n",
    "A temporary **Year +3** is generated programmatically only to determine the **Year +2 season outcome** (Promoted / Survived / Relegated). After outcome extraction, the Year +3 rows are dropped.\n",
    "\n",
    "**Granularity** is one row per team × season × relegation event. So one relegation event (e.g., Burnley 2023-24) is represented by four rows (Year -1 through Year +2).\n",
    "\n",
    "## Data sources\n",
    "\n",
    "### Attendance data\n",
    "Attendance values were manually compiled into an Excel workbook using:\n",
    "\n",
    "- **European-Football-Statistics.co.uk** (season-level attendances)\n",
    "\n",
    "Because COVID-affected seasons are not comparable to normal seasons, I sometimes entered the string value `COVID` into attendance cells during manual compilation. The pipeline converts these placeholders to missing values (`pd.NA`) before numeric conversion.\n",
    "\n",
    "[Download Relegation Attendance Churn.xlsx](../../assets/files/datasets/Relegation%20Attendance%20Churn_copy.xlsx)\n",
    "\n",
    "### League position and performance\n",
    "League tier, final position, and raw season performance totals come from:\n",
    "\n",
    "- **The Fjelstul English Football Database (standings dataset)**  \n",
    "  Fjelstul, Joshua C. “The Fjelstul English Football Database v1.1.0.” May 26, 2024. https://www.github.com/jfjelstul/englishfootball  \n",
    "\n",
    "> Licensing note: The Fjelstul database is distributed under **CC-BY-SA 4.0**. This derived dataset is made available under the same **CC BY-SA 4.0** license.\n",
    "\n",
    "- **Patch file**  \n",
    "The Fjelstul dataset does not include the 2024/25 season, so I have supplemented it with a patch file (`standings24_25.csv`) to include the most recent completed season.\n",
    "\n",
    "[Download standings24_25.csv](../../assets/files/datasets/standings24_25.csv)\n",
    "\n",
    "## Data dictionary\n",
    "\n",
    "### Identifiers and structure\n",
    "\n",
    "| Column | Type | Description |\n",
    "|------|------|-------------|\n",
    "| Relegation_Event_ID | string | Unique ID for a relegation event (`\"{Team} {Season}\"`) |\n",
    "| Team | string | Standardized club name (matches standings naming) |\n",
    "| Season | string | Season in `YYYY-YY` format (e.g., `2002-03`) |\n",
    "| Year_vs_Relegation | int | Relative season index: `-1`, `0`, `1`, `2` |\n",
    "\n",
    "### Attendance\n",
    "\n",
    "| Column | Type | Description |\n",
    "|------|------|-------------|\n",
    "| Attendance | int (nullable) | Average home league attendance. Missing where unusable (e.g. COVID). |\n",
    "\n",
    "### League status and outcome\n",
    "\n",
    "| Column | Type | Description |\n",
    "|------|------|-------------|\n",
    "| Tier | int (nullable) | League tier (`1` = top flight, `2` = Championship, etc.) |\n",
    "| Position | int (nullable) | Final league position |\n",
    "| Year_End_Outcome | string | `Promoted`, `Survived`, or `Relegated` |\n",
    "\n",
    "### Raw season performance totals\n",
    "\n",
    "| Column | Type | Description |\n",
    "|------|------|-------------|\n",
    "| Games_Played_History | int (nullable) | League matches played |\n",
    "| Wins_History | int (nullable) | League wins |\n",
    "| Goals_For_History | int (nullable) | League goals scored |\n",
    "| Goals_Against_History | int (nullable) | League goals conceded |\n",
    "| Points_History | int (nullable) | League points total |\n",
    "\n",
    "## Inputs and outputs\n",
    "\n",
    "::: {.panel-tabset}\n",
    "## Inputs\n",
    "| File | Purpose |\n",
    "|---|---|\n",
    "| [Relegation Attendance Churn.xlsx](../../assets/files/datasets/Relegation%20Attendance%20Churn_copy.xlsx) | Wide-format attendance sheet |\n",
    "| [Standings24_25.csv](../../assets/files/datasets/standings24_25.csv) | Patch file for 24/25 season |\n",
    "\n",
    "## Outputs\n",
    "| File | Purpose |\n",
    "|---|---|\n",
    "| [perfect_tidy_data.csv](../../assets/files/datasets/perfect_tidy_data.csv) | Final tidy dataset |\n",
    ":::\n",
    "\n",
    "## Dataset design\n",
    "\n",
    "### Primary identifier: `Relegation_Event_ID`\n",
    "\n",
    "A relegation event is uniquely identified as:\n",
    "\n",
    "`Relegation_Event_ID = \"{Team} {RelegationSeason}\"`\n",
    "\n",
    "Example: `Burnley 2023-24`\n",
    "\n",
    "This identifier is repeated across the four rows representing Year -1 through Year +2.\n",
    "\n",
    "## Reshaping pipeline\n",
    "\n",
    "### Step 0: Prerequisites\n",
    "``` python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# You may need to install thefuzz\n",
    "pip install thefuzz\n",
    "from thefuzz import process\n",
    "```\n",
    "\n",
    "### Step 1 — Load the wide spreadsheet and clean attendance columns\n",
    "\n",
    "**Why:** The attendance sheet is wide-format, and attendance values include commas and placeholders (e.g., `COVID`).\n",
    "\n",
    "**What happens:**\n",
    "- Strip commas (e.g., `23,721` → `23721`)\n",
    "- Convert `COVID` and `nan` placeholders to missing values (`pd.NA`)\n",
    "- Coerce to numeric\n",
    "\n",
    "``` python\n",
    "attendance_cols = [c for c in df_wide.columns if \"Attendance\" in c]\n",
    "for col in attendance_cols:\n",
    "    df_wide[col] = df_wide[col].astype(str).str.replace(',', '').replace('COVID', pd.NA).replace('nan', pd.NA)\n",
    "    df_wide[col] = pd.to_numeric(df_wide[col], errors='coerce')\n",
    "```\n",
    "\n",
    "### Step 2 — Load standings data (for auditing + enrichment)\n",
    "\n",
    "**Why:** Team naming mismatches and season key mismatches can silently break merges. The standings dataset provides a stable reference list of teams and seasons.\n",
    "\n",
    "**What happens:**\n",
    "- Load standings.csv\n",
    "- Optionally append standings24_25.csv\n",
    "- Rename and select relevant columns\n",
    "- Coerce numeric columns\n",
    "\n",
    "``` python\n",
    "try:\n",
    "    # Load 'season' as string to be safe\n",
    "    df_history = pd.read_csv('../../assets/files/datasets/standings.csv', dtype={'season': str})\n",
    "\n",
    "    # --- Load patch file and combine ---\n",
    "    try:\n",
    "        df_patch = pd.read_csv('../../assets/files/datasets/standings24_25.csv', dtype={'season': str})\n",
    "        # Append patch data to history data\n",
    "        df_history = pd.concat([df_history, df_patch], ignore_index=True)\n",
    "        print(\"Step 2a: Loaded and applied 'standings24_25.csv'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Step 2a: 'standings24_25.csv' not found. Skipping patch.\")\n",
    "    # --- End new patch logic ---\n",
    "\n",
    "    print(\"Step 2b: Loaded historical data for auditing.\")\n",
    "\n",
    "    # Create the cleaned df_positions here for later use\n",
    "    df_positions = df_history.rename(columns={\n",
    "        'team_name': 'Team',\n",
    "        'season': 'Season',\n",
    "        'position': 'Position_History',\n",
    "        'played': 'Games_Played_History',\n",
    "        'wins': 'Wins_History',\n",
    "        'goals_for': 'Goals_For_History',\n",
    "        'goals_against': 'Goals_Against_History',\n",
    "        'points': 'Points_History',\n",
    "        'tier': 'Tier_History'\n",
    "    })[['Team', 'Season', 'Position_History', 'Games_Played_History', 'Wins_History',\n",
    "        'Goals_For_History', 'Goals_Against_History', 'Points_History', 'Tier_History']]\n",
    "\n",
    "    # Convert new history columns to numeric\n",
    "    numeric_cols = ['Position_History', 'Games_Played_History', 'Wins_History',\n",
    "                    'Goals_For_History', 'Goals_Against_History', 'Points_History', 'Tier_History']\n",
    "    for col in numeric_cols:\n",
    "        df_positions[col] = pd.to_numeric(df_positions[col], errors='coerce')\n",
    "```\n",
    "\n",
    "### Step 3 — Audit team names (fuzzy matching)\n",
    "\n",
    "**Why:** The manual spreadsheet uses informal names (Man City, Spurs, etc.). The merge requires standardized names.\n",
    "\n",
    "**What happens:**  \n",
    "- Identify team names in attendance sheet that don’t match standings\n",
    "- Propose high-confidence fuzzy matches (>85)\n",
    "- Warn on low-confidence values\n",
    "    \n",
    "``` python\n",
    "teams_in_wide_df = set(df_wide['Relegated Team'].astype(str).unique())\n",
    "    teams_in_history_df = set(df_positions['Team'].astype(str).unique())\n",
    "\n",
    "    unmatched_teams = teams_in_wide_df.difference(teams_in_history_df)\n",
    "\n",
    "    # Create an empty map to build automatically\n",
    "    auto_team_name_map = {}\n",
    "\n",
    "    if unmatched_teams:\n",
    "        print(\"--- AUDIT: TEAM NAME MISMATCHES FOUND ---\")\n",
    "        print(\"The following teams from your file do not match the history file.\")\n",
    "        print(\"Attempting to build an automatic mapping...\")\n",
    "\n",
    "        # Loop through each unique unmatched team and find the best match\n",
    "        for team in sorted(unmatched_teams):\n",
    "            # process.extractOne returns a tuple: (best_match, score)\n",
    "            suggestion = process.extractOne(team, teams_in_history_df)\n",
    "\n",
    "            # Auto-map spaces, asterisks, and high-confidence matches\n",
    "            if suggestion[1] > 85:\n",
    "                print(f\"  - Auto-mapping: \\\"{team}\\\" -> \\\"{suggestion[0]}\\\" (Score: {suggestion[1]})\")\n",
    "                auto_team_name_map[team] = suggestion[0]\n",
    "            else:\n",
    "                # Don't map low-confidence or junk data\n",
    "                if team not in ['nan', 'TEAM', 'Relegated Team']:\n",
    "                    print(\n",
    "                        f\"  - WARNING: \\\"{team}\\\" has no confident match. (Best: \\\"{suggestion[0]}\\\" at {suggestion[1]}%)\")\n",
    "                    print(f\"    -> Please add a fix for \\\"{team}\\\" to 'manual_overrides_map' in Step 4.\")\n",
    "\n",
    "        print(\"---------------------------------------------\")\n",
    "    else:\n",
    "        print(\"--- AUDIT: TEAM NAMES OK ---\")\n",
    "        print(\"All team names in your file match the history file.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: '../../assets/files/datasets/standings.csv' not found. Skipping audit and enrichment.\")\n",
    "    # Create empty dataframes if file not found\n",
    "    df_positions = pd.DataFrame(columns=['Team', 'Season', 'Position_History', 'Games_Played_History',\n",
    "                                         'Wins_History', 'Goals_For_History', 'Goals_Against_History',\n",
    "                                         'Points_History', 'Tier_History'])\n",
    "    auto_team_name_map = {}\n",
    "```\n",
    "### Step 4 — Standardize team names (auto-map + manual overrides)\n",
    "\n",
    "**Why:** Fuzzy matching is helpful but not perfect; known edge cases are fixed with manual overrides.\n",
    "\n",
    "**What happens:**\n",
    "- Merge the auto-map and manual overrides  \n",
    "- Replace names in the wide spreadsheet  \n",
    "- Filter out remaining junk/unmapped rows\n",
    "\n",
    "``` python\n",
    "manual_overrides_map = {\n",
    "    \"Man Utd\": \"Manchester United\",\n",
    "    \"Man City\": \"Manchester City\",\n",
    "    \"Spurs\": \"Tottenham Hotspur\",\n",
    "    \"QPR\": \"Queens Park Rangers\",\n",
    "    \"Wolves\": \"Wolverhampton Wanderers\",\n",
    "    \"Sheff Utd\": \"Sheffield United\",\n",
    "    \"Sheff Wed\": \"Sheffield Wednesday\",\n",
    "    \"Nott'm Forest\": \"Nottingham Forest\",\n",
    "    \"Notts County\": \"Nottingham Forest\"\n",
    "    # e.g., if audit warns about \"Bradford\", add:\n",
    "    # \"Bradford\": \"Bradford City\",\n",
    "}\n",
    "# Combine the auto-generated map with the manual overrides\n",
    "# This is where we combine the two dictionaries.\n",
    "# The 'manual_overrides_map' will overwrite any conflicting keys from 'auto_team_name_map'.\n",
    "final_team_map = {**auto_team_name_map, **manual_overrides_map}\n",
    "\n",
    "df_wide['Relegated Team'] = df_wide['Relegated Team'].replace(final_team_map)\n",
    "print(\"Step 4: Standardized 'Relegated Team' names using auto-mapping and manual overrides.\")\n",
    "# --- 4.5. FILTERING STEP ---\n",
    "# Now that names are mapped, we can filter out junk rows that didn't get mapped\n",
    "teams_in_history_df = set(df_positions['Team'].astype(str).unique())\n",
    "df_wide = df_wide[df_wide['Relegated Team'].isin(teams_in_history_df)]\n",
    "print(\"Step 4.5: Filtered out junk rows (e.g., 'nan', 'TEAM').\")\n",
    "```\n",
    "\n",
    "### Step 5 — Create a stable event key (Relegation_Event_ID)\n",
    "\n",
    "**Why:** Each relegation event needs a stable identifier across Year -1..+2.\n",
    "\n",
    "``` python\n",
    "df_wide['Relegation_Event_ID'] = df_wide['Relegated Team'] + ' ' + df_wide['Season']\n",
    "print(\"Step 5: Created 'Relegation_Event_ID'.\")\n",
    "```\n",
    "### Step 6 — Convert wide → tidy (safe slices + concat)\n",
    "\n",
    "**Why:** The attendance sheet is wide and not safely meltable without careful column mapping.\n",
    "\n",
    "**What happens:**  \n",
    "- Create four normalized slices (Year -1..+2)  \n",
    "- Concatenate into one long table with a consistent schema  \n",
    "\n",
    "``` python\n",
    "## Helper function to create each slice\n",
    "def make_slice(df, team_col, season_col, att_col, year_vs):\n",
    "    # --- Check if att_col exists, otherwise use pd.NA ---\n",
    "    # This makes the function robust for the missing Y3 attendance\n",
    "    if att_col not in df.columns:\n",
    "        # Create a temporary column of NAs if the attendance col doesn't exist\n",
    "        df = df.assign(Attendance_tmp=pd.NA) \n",
    "    else:\n",
    "        # If the col *does* exist, rename it\n",
    "        df = df.rename(columns={att_col: \"Attendance_tmp\"})\n",
    "        \n",
    "    slice_df = df.rename(columns={\n",
    "        team_col: \"Team\",\n",
    "        season_col: \"Season_tmp\"\n",
    "    }).assign(Year_vs_Relegation=year_vs)\n",
    "\n",
    "    # Select and rename final columns\n",
    "    slice_df = slice_df[['Relegation_Event_ID', 'Team', 'Season_tmp', 'Attendance_tmp', 'Year_vs_Relegation']].copy()\n",
    "    slice_df.columns = ['Relegation_Event_ID', 'Team', 'Season', 'Attendance', 'Year_vs_Relegation']\n",
    "\n",
    "    return slice_df\n",
    "\n",
    "# Create slices (Only up to Year 2)\n",
    "slice_minus1 = make_slice(df_wide, \"Relegated Team\", \"Year Before\", \"Year Before Att\", -1)\n",
    "slice_0      = make_slice(df_wide, \"Relegated Team\", \"Season\", \"Attendance\", 0)\n",
    "slice_1      = make_slice(df_wide, \"Relegated Team\", \"Year After\", \"Attendance year after\", 1)\n",
    "slice_2      = make_slice(df_wide, \"Relegated Team\", \"2 years after\", \"Attendance 2 years after\", 2)\n",
    "\n",
    "# Concatenate slices safely\n",
    "df_tidy = pd.concat([slice_minus1, slice_0, slice_1, slice_2], ignore_index=True)\n",
    "\n",
    "print(\"Step 6: Data has been 'tidied' successfully!\")\n",
    "```\n",
    "\n",
    "### Step 7 — Standardize season keys (YYYY-YY)\n",
    "\n",
    "- Why: Season formats vary (e.g., 1991-1992 vs 1992-93) and must align for joins.\n",
    "\n",
    "``` python\n",
    "# --- HELPER FUNCTION 1 ---\n",
    "def format_season(season_str):\n",
    "    \"\"\"Converts '1991-1992' to '1991-92' and leaves '1992-93' as is.\"\"\"\n",
    "    if pd.isna(season_str):\n",
    "        return pd.NA\n",
    "    parts = str(season_str).split('-')\n",
    "    if len(parts) == 2:\n",
    "        if len(parts[1]) == 4:  # Format is '1991-1992'\n",
    "            return f\"{parts[0]}-{parts[1][-2:]}\"\n",
    "        else:  # Format is '1992-93'\n",
    "            return season_str\n",
    "    return season_str  # Return as-is if not in expected format\n",
    "\n",
    "# --- HELPER FUNCTION 2 ---\n",
    "def increment_season(season_str):\n",
    "    \"\"\"Converts a season string like '1995-96' to '1996-97'.\"\"\"\n",
    "    if pd.isna(season_str):\n",
    "        return pd.NA\n",
    "    try:\n",
    "        start_year = int(season_str.split('-')[0])\n",
    "        next_start_year = start_year + 1\n",
    "        \n",
    "        # Handle the '1999-00' case\n",
    "        if next_start_year == 1999:\n",
    "            return \"1999-00\"\n",
    "        \n",
    "        next_end_year_short = str(next_start_year + 1)[-2:] # e.g., 97\n",
    "        return f\"{next_start_year}-{next_end_year_short}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error incrementing season '{season_str}': {e}\")\n",
    "        return pd.NA\n",
    "\n",
    "# Apply the formatting\n",
    "df_tidy['Season'] = df_tidy['Season'].apply(format_season)\n",
    "print(\"Step 7: Standardized 'Season' key to 'YYYY-YY' format (e.g., '1992-93').\")\n",
    "```\n",
    "\n",
    "### Step 7.5 — Temporary Year +3 rows (outcome extraction only)\n",
    "\n",
    "**Why:** To compute the outcome of Year +2, we need to observe the next season’s tier transition.\n",
    "\n",
    "``` python\n",
    "print(\"Step 7.5: Generating temporary Year 3 rows for outcome calculation...\")\n",
    "\n",
    "# 1. Find all Year 2 rows\n",
    "df_y2_rows = df_tidy[df_tidy['Year_vs_Relegation'] == 2].copy()\n",
    "\n",
    "# 2. Transform them into Year 3 rows\n",
    "df_y2_rows['Year_vs_Relegation'] = 3\n",
    "df_y2_rows['Attendance'] = pd.NA # Attendance data is not needed\n",
    "df_y2_rows['Season'] = df_y2_rows['Season'].apply(increment_season)\n",
    "\n",
    "# 3. Concatenate these helper rows back onto the main tidy dataframe\n",
    "df_tidy = pd.concat([df_tidy, df_y2_rows], ignore_index=True)\n",
    "print(\"Step 7.5: Temporary Year 3 rows created and added.\")\n",
    "```\n",
    "\n",
    "### Step 8 — Enrich tidy data with standings (raw totals)\n",
    "\n",
    "**Why:** Add tier, position, and season totals for later analysis while keeping this dataset “raw”.\n",
    "\n",
    "``` python\n",
    "if not df_positions.empty:\n",
    "    # --- Upgrade the history file's 'Season' column ---\n",
    "    season_numeric = pd.to_numeric(df_positions['Season'], errors='coerce').dropna()\n",
    "    season_end_year = (season_numeric + 1).astype(int).astype(str).str.zfill(4).str[-2:]\n",
    "    df_positions.loc[season_numeric.index, 'Season'] = season_numeric.astype(int).astype(str) + '-' + season_end_year\n",
    "    print(\"Step 8a: Upgraded history file 'Season' key.\")\n",
    "\n",
    "    # --- MODIFICATION ---\n",
    "    # We are NO LONGER calculating per-game metrics here.\n",
    "    # We are ONLY merging the raw values.\n",
    "    df_positions_merge = df_positions[[\n",
    "        'Team', 'Season', 'Position_History', 'Tier_History',\n",
    "        'Games_Played_History', 'Wins_History', 'Goals_For_History', \n",
    "        'Goals_Against_History', 'Points_History'\n",
    "    ]]\n",
    "    # --- END MODIFICATION ---\n",
    "\n",
    "    # --- Merge the data ---\n",
    "    print(\"Step 8b: Merging tidy data with position data...\")\n",
    "    df_final = pd.merge(\n",
    "        df_tidy,\n",
    "        df_positions_merge,  # Use the merge-ready dataframe with raw columns\n",
    "        on=['Team', 'Season'],\n",
    "        how='left'\n",
    "    )\n",
    "    df_final['Position'] = df_final['Position_History']\n",
    "    df_final['Tier'] = df_final['Tier_History']\n",
    "    print(\"Step 8: Data enriched. 'df_final' created and 'Tier' column added.\")\n",
    "\n",
    "else:\n",
    "    print(\"Step 8: Skipping merge as 'standings.csv' was not loaded.\")\n",
    "    df_final = df_tidy.copy() \n",
    "    # Add empty columns\n",
    "    for col in ['Position', 'Tier', 'Games_Played_History', 'Wins_History', 'Goals_For_History', 'Goals_Against_History', 'Points_History']:\n",
    "        df_final[col] = pd.NA\n",
    "```\n",
    "\n",
    "### Step 8.5 — Compute Year_End_Outcome from tier transitions\n",
    "**Why:** Outcomes (Promoted/Survived/Relegated) are derived from how tier changes year-to-year within each event.  \n",
    "\n",
    "``` python\n",
    "print(\"Step 8.5: Calculating 'Year_End_Outcome' based on precise timeline...\")\n",
    "\n",
    "# 1. Calculate the Tier Change (Tier_Next_Year - Tier_Current_Year)\n",
    "df_final['Tier_Next_Year'] = df_final.groupby('Relegation_Event_ID')['Tier'].shift(-1)\n",
    "df_final['Tier_Change'] = df_final['Tier_Next_Year'] - df_final['Tier']\n",
    "\n",
    "# 2. Map the Tier Change to the Outcome Label\n",
    "def map_tier_change_to_outcome(row):\n",
    "    \"\"\"Maps the outcome based on the user's exact timeline definition.\"\"\"\n",
    "    \n",
    "    year_vs_relegation = row['Year_vs_Relegation'] \n",
    "    current_tier = row['Tier']\n",
    "    tier_change = row['Tier_Change']\n",
    "\n",
    "    # --- 1. Handle Year -1 (Based on CURRENT Tier) ---\n",
    "    if year_vs_relegation == -1:\n",
    "        if current_tier == 1:\n",
    "            return 'Survived'\n",
    "        if current_tier == 2:\n",
    "            return 'Promoted'\n",
    "        return pd.NA\n",
    "\n",
    "    # --- 2. Handle Year 0 (The Relegation Event) ---\n",
    "    if year_vs_relegation == 0:\n",
    "        return 'Relegated'\n",
    "\n",
    "    # --- 3. Handle Year +1 and +2 (Based on TIER CHANGE) ---\n",
    "    if year_vs_relegation in [1, 2]:\n",
    "        if pd.isna(tier_change):\n",
    "            return pd.NA \n",
    "        \n",
    "        if tier_change == -1:\n",
    "            return 'Promoted'\n",
    "        if tier_change == 0:\n",
    "            return 'Survived'\n",
    "        if tier_change == 1:\n",
    "            return 'Relegated'\n",
    "        \n",
    "        return pd.NA\n",
    "\n",
    "    return pd.NA # Catches Y3 row\n",
    "\n",
    "# Apply the function\n",
    "df_final['Year_End_Outcome'] = df_final.apply(map_tier_change_to_outcome, axis=1)\n",
    "print(\"Step 8.5: 'Year_End_Outcome' column successfully created.\")\n",
    "```\n",
    "\n",
    "### Step 9 — Finalize schema and export  \n",
    "**What happens:**  \n",
    "- Cast numeric fields to nullable integers  \n",
    "- Drop helper Year +3 rows  \n",
    "- Select final column set and sort  \n",
    "- Write perfect_tidy_data.csv\n",
    "\n",
    "``` python\n",
    "print(\"Step 9: Polishing final data...\")\n",
    "\n",
    "# --- MODIFICATION: Added all _History columns ---\n",
    "final_columns = [\n",
    "    'Relegation_Event_ID', 'Team', 'Season', 'Tier',\n",
    "    'Position', 'Attendance', 'Year_vs_Relegation', 'Year_End_Outcome',\n",
    "    'Games_Played_History', 'Wins_History', 'Goals_For_History', \n",
    "    'Goals_Against_History', 'Points_History'\n",
    "]\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "# Ensure final columns exist before slicing\n",
    "for col in final_columns:\n",
    "    if col not in df_final.columns:\n",
    "        df_final[col] = pd.NA\n",
    "\n",
    "# --- MODIFICATION: Cast all numeric columns to Int64 ---\n",
    "for col in ['Attendance', 'Position', 'Tier', 'Games_Played_History', 'Wins_History', 'Goals_For_History', 'Goals_Against_History', 'Points_History']:\n",
    "    df_final[col] = pd.to_numeric(df_final[col], errors='coerce').astype('Int64')\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "# --- DROP THE HELPER ROW ---\n",
    "df_final = df_final[df_final['Year_vs_Relegation'] != 3].copy()\n",
    "\n",
    "# Keep only the final columns and sort\n",
    "df_final = df_final[final_columns].sort_values(by=['Relegation_Event_ID', 'Year_vs_Relegation'])\n",
    "\n",
    "# Save the final, perfect data to a new CSV in the 'datasets' folder\n",
    "output_path = \"../../assets/files/datasets/perfect_tidy_data.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "import os\n",
    "print(f\"Step 9: Success! Final table saved to '{output_path}'\")\n",
    "#| output: false\n",
    "```\n",
    "\n",
    "## From tidy data to analysis-ready tables\n",
    "While that completes the reshaping pipeline, further steps are needed to prepare the data for analysis and visualization.\n",
    "\n",
    "``` python\n",
    "print(\"--- PREPARING DATA FOR ANALYSIS ---\")\n",
    "# --- ANALYSIS PREP ---\n",
    "# This block creates the 'df_plot' DataFrame for all visualizations\n",
    "# 1. Get the baseline attendance (Year 0) for each event\n",
    "df_y0_baseline = df_final[\n",
    "    df_final['Year_vs_Relegation'] == 0\n",
    "][['Relegation_Event_ID', 'Attendance']].rename(columns={'Attendance': 'Attendance_Y0_Baseline'})\n",
    "\n",
    "# 2. Merge this baseline back onto the main df\n",
    "df_plot = pd.merge(df_final, df_y0_baseline, on='Relegation_Event_ID', how='left')\n",
    "\n",
    "# 3. Calculate the Pct Change vs. Year 0 for all years\n",
    "df_plot['Pct_Change_vs_Y0'] = (\n",
    "    (df_plot['Attendance'] - df_plot['Attendance_Y0_Baseline']) / \n",
    "    df_plot['Attendance_Y0_Baseline']\n",
    ")\n",
    "\n",
    "# 4. Get the *key outcome* from Year 1 (the first post-relegation season)\n",
    "df_y1_outcome = df_final[\n",
    "    df_final['Year_vs_Relegation'] == 1\n",
    "][['Relegation_Event_ID', 'Year_End_Outcome']].rename(columns={'Year_End_Outcome': 'Outcome_Group'})\n",
    "\n",
    "# 5. Merge this outcome group onto the whole plot dataset\n",
    "df_plot = pd.merge(df_plot, df_y1_outcome, on='Relegation_Event_ID', how='left')\n",
    "\n",
    "\n",
    "# 5.5: IMPUTE 2024-25 OUTCOMES ---\n",
    "# The Y+1 outcome for the 2023-24 cohort is NA. We impute it manually.\n",
    "print(\"Imputing outcomes for 2023-24 cohort...\")\n",
    "\n",
    "# Define our imputations based on likely 2024-25 season-end performance\n",
    "impute_map = {\n",
    "    'Burnley 2023-24': 'Promoted',\n",
    "    'Sheffield United 2023-24': 'Survived', # Assuming playoffs = Survived\n",
    "    'Luton Town 2023-24': 'Relegated'\n",
    "}\n",
    "\n",
    "# Apply the imputation\n",
    "# This finds rows where Outcome_Group is NA and maps the Event ID to our new value\n",
    "df_plot['Outcome_Group'] = df_plot['Outcome_Group'].fillna(\n",
    "    df_plot['Relegation_Event_ID'].map(impute_map)\n",
    ")\n",
    "print(\"Imputation complete.\")\n",
    "\n",
    "# 6. Convert Pct_Change to a percentage number (e.g., 0.05 -> 5)\n",
    "df_plot['Pct_Change_vs_Y0_Display'] = df_plot['Pct_Change_vs_Y0'] * 100\n",
    "\n",
    "# 7. Calculate Per-Game metrics on the fly\n",
    "df_plot['Goals_For_Per_Game'] = df_plot['Goals_For_History'] / df_plot['Games_Played_History']\n",
    "df_plot['Points_Per_Game'] = df_plot['Points_History'] / df_plot['Games_Played_History']\n",
    "\n",
    "# 8. Define the FINAL anomaly list\n",
    "# We are KEEPING Wimbledon, as it's a valid case study.\n",
    "anomaly_events = [\n",
    "    'Middlesbrough 1992-93', # Rebuild\n",
    "    'Sunderland 1996-97',    # New stadium\n",
    "    'Leicester City 2001-02', # New stadium\n",
    "    'Bolton Wanderers 1995-96'  # New stadium (in Y+2)\n",
    "]\n",
    "\n",
    "# 9. Create the FINAL filtered DataFrame for plotting\n",
    "df_plot_filtered = df_plot[~df_plot['Relegation_Event_ID'].isin(anomaly_events)]\n",
    "print(f\"Removed {len(df_plot) - len(df_plot_filtered)} total data points for structural anomalies.\")\n",
    "\n",
    "# 10. Create the cleaner plot labels\n",
    "label_map = {\n",
    "    'Promoted': 'Promoted (Back in PL)',\n",
    "    'Survived': 'Survived (Still in Champ)',\n",
    "    'Relegated': 'Relegated (Down to L1)'\n",
    "}\n",
    "df_plot['Plot_Group_Label'] = df_plot['Outcome_Group'].map(label_map)\n",
    "df_plot_filtered['Plot_Group_Label'] = df_plot_filtered['Outcome_Group'].map(label_map)\n",
    "\n",
    "print(\"--- Analysis data is ready. ---\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Analysis prep script (full, copy-paste ready)\n",
    "``` python\n",
    "## From tidy data to analysis-ready tables\n",
    "print(\"--- PREPARING DATA FOR ANALYSIS ---\")\n",
    "\n",
    "# --- ANALYSIS PREP ---\n",
    "# This block creates the 'df_plot' DataFrame for all visualizations\n",
    "# 1. Get the baseline attendance (Year 0) for each event\n",
    "df_y0_baseline = df_final[\n",
    "    df_final['Year_vs_Relegation'] == 0\n",
    "][['Relegation_Event_ID', 'Attendance']].rename(columns={'Attendance': 'Attendance_Y0_Baseline'})\n",
    "\n",
    "# 2. Merge this baseline back onto the main df\n",
    "df_plot = pd.merge(df_final, df_y0_baseline, on='Relegation_Event_ID', how='left')\n",
    "\n",
    "# 3. Calculate the Pct Change vs. Year 0 for all years\n",
    "df_plot['Pct_Change_vs_Y0'] = (\n",
    "    (df_plot['Attendance'] - df_plot['Attendance_Y0_Baseline']) / \n",
    "    df_plot['Attendance_Y0_Baseline']\n",
    ")\n",
    "\n",
    "# 4. Get the *key outcome* from Year 1 (the first post-relegation season)\n",
    "df_y1_outcome = df_final[\n",
    "    df_final['Year_vs_Relegation'] == 1\n",
    "][['Relegation_Event_ID', 'Year_End_Outcome']].rename(columns={'Year_End_Outcome': 'Outcome_Group'})\n",
    "\n",
    "# 5. Merge this outcome group onto the whole plot dataset\n",
    "df_plot = pd.merge(df_plot, df_y1_outcome, on='Relegation_Event_ID', how='left')\n",
    "\n",
    "\n",
    "# 5.5: IMPUTE 2024-25 OUTCOMES ---\n",
    "# The Y+1 outcome for the 2023-24 cohort is NA. We impute it manually.\n",
    "print(\"Imputing outcomes for 2023-24 cohort...\")\n",
    "\n",
    "# Define our imputations based on likely 2024-25 season-end performance\n",
    "impute_map = {\n",
    "    'Burnley 2023-24': 'Promoted',\n",
    "    'Sheffield United 2023-24': 'Survived', # Assuming playoffs = Survived\n",
    "    'Luton Town 2023-24': 'Relegated'\n",
    "}\n",
    "\n",
    "# Apply the imputation\n",
    "# This finds rows where Outcome_Group is NA and maps the Event ID to our new value\n",
    "df_plot['Outcome_Group'] = df_plot['Outcome_Group'].fillna(\n",
    "    df_plot['Relegation_Event_ID'].map(impute_map)\n",
    ")\n",
    "print(\"Imputation complete.\")\n",
    "\n",
    "# 6. Convert Pct_Change to a percentage number (e.g., 0.05 -> 5)\n",
    "df_plot['Pct_Change_vs_Y0_Display'] = df_plot['Pct_Change_vs_Y0'] * 100\n",
    "\n",
    "# 7. Calculate Per-Game metrics on the fly\n",
    "df_plot['Goals_For_Per_Game'] = df_plot['Goals_For_History'] / df_plot['Games_Played_History']\n",
    "df_plot['Points_Per_Game'] = df_plot['Points_History'] / df_plot['Games_Played_History']\n",
    "\n",
    "# 8. Define the FINAL anomaly list\n",
    "# We are KEEPING Wimbledon, as it's a valid case study.\n",
    "anomaly_events = [\n",
    "    'Middlesbrough 1992-93', # Rebuild\n",
    "    'Sunderland 1996-97',    # New stadium\n",
    "    'Leicester City 2001-02', # New stadium\n",
    "    'Bolton Wanderers 1995-96'  # New stadium (in Y+2)\n",
    "]\n",
    "\n",
    "# 9. Create the FINAL filtered DataFrame for plotting\n",
    "df_plot_filtered = df_plot[~df_plot['Relegation_Event_ID'].isin(anomaly_events)]\n",
    "print(f\"Removed {len(df_plot) - len(df_plot_filtered)} total data points for structural anomalies.\")\n",
    "\n",
    "# 10. Create the cleaner plot labels\n",
    "label_map = {\n",
    "    'Promoted': 'Promoted (Back in PL)',\n",
    "    'Survived': 'Survived (Still in Champ)',\n",
    "    'Relegated': 'Relegated (Down to L1)'\n",
    "}\n",
    "df_plot['Plot_Group_Label'] = df_plot['Outcome_Group'].map(label_map)\n",
    "df_plot_filtered['Plot_Group_Label'] = df_plot_filtered['Outcome_Group'].map(label_map)\n",
    "\n",
    "print(\"--- Analysis data is ready. ---\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diggydigsdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
