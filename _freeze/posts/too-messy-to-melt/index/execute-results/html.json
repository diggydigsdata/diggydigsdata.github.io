{
  "hash": "4db53d90f45fb8433cffd214743c5ef6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Too Messy to Melt\"\nauthor: \"Diggy\"\ndate: \"2025-11-01\"\ndescription: \"Getting an analysis-ready dataset from a tabular trashcan\"\nimage: \"featured.png\"\nimage-alt: \"Image showing a sprawling and chaotic excel spreadsheet on top of a neat and tidy pandas dataframe\"\ncategories: [Football, Analytics, Excel, Python, R, Tidy-Data]\nlicense: \"CC BY\"\nexecute:\n    echo: false\n    output: false\njupyter: python3\n---\n\n#### Coming Soon\n\n<!--\n#### Tidy Data\nOne of the driving forces behind some of the complex logic in this dataset was that I have adapted a project I started before beginning my MSc Data Science. I had never heard of Tidy Data, so I set about chucking data into a big Excel spreadsheet. Even when I learned about Tidy Data, my Excel brain still thought it seemed like a major hassle to create a big, long table and fiddle with pd.melt or dplyr. You can just make one wide table and be done with it. But when I came back to fix up this article and add some nice plots and analytics beyond my pivot tables, I didn't quite know where to start. It was such a mess and there were so many variables that wrapping my head around them was turning into a linear algebra problem. This data wasn't just untidy, it was a statistical cesspit. Normally, it's fairly straightforward to flip a dataset from wide to long, or from dirty to tidy. Pandas has pd.melt and R has TidyR. But my data was in such a mess that neither of those would be enough. Along the way, I ended up discovering much more robust ways to engineer the variables I needed and to ensure accuracy. So a technical walkthrough to tidy this tabular trashcan is documented here.\n-->\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}